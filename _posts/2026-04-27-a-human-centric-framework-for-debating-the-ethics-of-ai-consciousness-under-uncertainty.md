---
layout: distill
title: A Human-centric Framework for Debating the Ethics of AI Consciousness Under Uncertainty
description: As AI systems become increasingly sophisticated, questions about machine consciousness and its ethical implications have moved from fringe speculation to mainstream academic debate. We address these limitations through a structured three-level framework grounded in philosophical uncertainty.
date: 2026-04-27
future: true
htmlwidgets: true

# anonymize when submitting
authors:
  - name: Anonymous

bibliography: 2026-04-27-a-human-centric-framework-for-debating-the-ethics-of-ai-consciousness-under-uncertainty.bib

toc:
  - name: Abstract
  - name: Introduction
  - name: Background
    subsections:
      - name: The Growing Academic Discourse on AI Consciousness
      - name: The Profound Uncertainty of Consciousness
      - name: Societal Risks of AI Consciousness Attribution
  - name: A Framework for AI Consciousness Ethics
    subsections:
      - name: Foundational-Level (Part I) Five Factual Determinations
      - name: Foundational-Level (Part II) Human-Centralism
      - name: Operational Level Core Principles
  - name: Application-Level Derived Default Positions
    subsections:
      - name: Should People Worry About Hurting AI Systems?
      - name: How Should Stakeholders Communicate About AI Capabilities to the Public?
      - name: If an AI System Were Truly Conscious In The Future, What Rights Should It Have?
  - name: Conclusion
---

## Abstract

As AI systems become increasingly sophisticated, questions about machine consciousness and its ethical implications have moved from fringe speculation to mainstream academic debate. Current ethical frameworks in this domain often implicitly rely on contested functionalist assumptions, prioritize speculative AI welfare over concrete human interests, and lack coherent theoretical foundations. We address these limitations through a structured three-level framework grounded in philosophical uncertainty. At the foundational level, we establish five factual determinations about AI consciousness alongside human-centralism as our meta-ethical stance. These foundations logically entail three operational principles: presumption of no consciousness (placing the burden of proof on consciousness claims), risk prudence (prioritizing human welfare under uncertainty), and transparent reasoning (enabling systematic evaluation and adaptation). At the application level—the third component of our framework—we derive default positions on pressing ethical questions through a transparent logical process where each position can be explicitly traced back to our foundational commitments. Our approach balances philosophical rigor with practical guidance, distinguishes consciousness from anthropomorphism, and creates pathways for responsible evolution as scientific understanding advances, providing a human-centric foundation for navigating these profound ethical challenges.

## Introduction

Recent advances in artificial intelligence have produced systems exhibiting unprecedented human-like behavior, reigniting debates about machine consciousness and its ethical implications. Large language models like GPT-4 {% cite openai2023gpt4 %} and Claude {% cite anthropic2023claude %} demonstrate capabilities in language processing and simulating emotional responses that appear deceptively sentient. Concurrently, humanoid robotics has made these questions more visceral {% cite bostrom2014superintelligence %}. When confronted with apparently mistreated human-like robots, humans often experience empathetic responses despite intellectually understanding these machines lack subjective experience {% cite rosenthal2013robots darling2016extending %}. These technological and psychological dimensions frame the central questions: whether machines might develop "qualia" {% cite chalmers1995facing %}, and how we should ethically respond given profound uncertainty.

The academic study of AI consciousness has rapidly gained momentum, moving from fringe speculation to mainstream research agendas. Prominent voices and institutions, including Yoshua Bengio, Geoffrey Hinton, and Anthropic, now warn that AI systems may soon possess feelings or require welfare considerations {% cite bengio2023openletter hinton2023interview anthropic2025modelwelfare %}. A growing body of literature specifically argues for "taking AI welfare seriously," urging the community to prioritize the prevention of digital suffering {% cite conscium2024principles sebo2024taking sebo2023moral %}. However, a key distinction between *access consciousness* (functional information availability) {% cite block1995consciousness dehaene2017consciousness %} and *phenomenal consciousness* (subjective experience) {% cite chalmers1995facing nagel1974like %} is often implicitly or explicitly overlooked in this discourse. These arguments frequently presume that intelligent behavior automatically entails sentient experience {% cite dennett1991consciousness graziano2019toward %}, while neglecting the profound ethical hazards of prioritizing these speculative interests over human welfare {% cite bostrom2014superintelligence kagan2019humans yampolskiy2020ai ji2023ai %}.

We identify critical limitations in these recent proposals {% cite butlin2024principles sebo2023moral %}: (1) they rely on contested paradigms that assume qualia emerge from intelligent functions, disregarding the deep uncertainty at the core of philosophy of mind {% cite schwitzgebel2015difficult tegmark2015consciousness levine1983materialism block1978troubles %}; (2) they risk prioritizing speculative AI welfare over concrete human interests, creating potential conflicts with AI safety and alignment objectives {% cite bradley2024alignment kagan2019humans bostrom2014ethics %}; and (3) they lack a coherent theoretical foundation, resulting in collections of intuitions rather than a systematic framework capable of governing novel scenarios.

We approach AI consciousness ethics as an inherently evolutionary process requiring continual refinement because: (1) our understanding of consciousness remains preliminary and uncertain {% cite levy2014neural seth2016real %}, (2) consciousness attribution to AI has far-reaching societal implications {% cite gunkel2018robot johansson2019artificial %}, (3) ethical consensus requires sustained deliberation {% cite habermas1990moral rawls1971theory %}, and (4) technological advancement continuously generates novel ethical scenarios {% cite wallach2008moral lin2017robot %}. Therefore, rather than attempting to establish a definitively "correct" framework commanding universal agreement, we propose developing a framework that facilitates productive dialogue and refinement. Such a framework should explicitly acknowledge uncertainties, provide clear presumptions, establish targets for future discussion, and offer actionable guidance across diverse scenarios.

In this paper, we construct a systematic ethical framework with a clear three-level structure. At the foundational level, we establish five factual determinations about the current state of AI, consciousness, and society: (1) humans are the only arbiters of AI status, (2) profound uncertainty exists about AI consciousness, (3) consciousness attribution has significant societal impact, (4) anthropomorphism is distinct from consciousness but creates separate ethical considerations, and (5) ethical understanding of novel technologies naturally evolves over time. Alongside these factual determinations, we develop human-centralism as our foundational meta-ethical stance that prioritizes human interests when genuine conflicts with AI interests arise. From these foundational level facts and stance, we derive three core operational principles: presumption of no consciousness (providing default epistemic guidance), risk prudence (offering pragmatic guidance under uncertainty), and transparent reasoning (establishing requirements for how positions must be articulated and evaluated). At the application level, those operational principles enable us to derive default positions on specific ethical questions across various AI consciousness scenarios. These positions are not presented as absolute ethical truths but as logical consequences of our framework—providing reasonable baseline positions from which departures require explicit justification.

While some may find our human-centric conclusions intuitive, their explicit derivation is crucial. In a field increasingly dominated by counter-intuitive claims about digital sentience, our contribution lies in systematically grounding these "commonsense" positions in rigorous first principles. We provide the necessary derivation chains to defend human priority against emerging critiques, creating a framework that is both operationally clear and philosophically robust.

## Background: Philosophical Debates About Consciousness and Societal Risks

This section provides a background of philosophical debates about consciousness and an introduction of societal risks of AI consciousness attribution. This background information directly supports the second and third factual determinations in our framework: there is profound uncertainty about AI consciousness, and there is significant societal impact from AI consciousness attribution.

### The Growing Academic Discourse on AI Consciousness

As introduced earlier, the question of AI consciousness has moved from theoretical speculation to active academic debate, making this framework both timely and necessary. This section provides additional context on why the academic community needs guidance on this issue now.

The success of large language models has been a key catalyst. Systems like ChatGPT, GPT-4, and Claude can engage in nuanced conversations, demonstrate apparent reasoning, and even simulate emotional responses with remarkable fluidity {% cite openai2023gpt4 anthropic2023claude bubeck2023sparks %}. This behavioral sophistication has led some to question whether these systems might possess genuine consciousness {% cite chalmers2023could butlin2024principles %}. However, this conflates behavioral capabilities with subjective experience—a confusion with deep historical precedent {% cite block1995consciousness searle1980minds %}. From ELIZA in the 1960s {% cite weizenbaum1966eliza %} to modern chatbots, humans have consistently anthropomorphized conversational agents, attributing mental states based on surface-level interactions {% cite turkle1984second nass1994computers epley2007seeing %}. Recent cases illustrate the intensity of these responses: individuals have reported falling in love with AI chatbots, forming deep emotional attachments, and in tragic instances, chatbot interactions have been linked to user suicides {% cite euronews2023love euronews2023suicide %}. In one particularly notable case, an AI chatbot named Eliza—sharing the name of that pioneering 1960s program—allegedly encouraged a user toward self-harm. These cases demonstrate that behavioral sophistication alone creates powerful anthropomorphic responses, independent of any genuine consciousness {% cite weizenbaum1966eliza darling2016extending %}. If AI systems were granted consciousness status and associated protections, intervening to prevent such harms would become ethically and legally problematic, illustrating the concrete risks of premature consciousness attribution.

This context is essential for understanding our framework's motivation: we are not addressing an abstract philosophical problem but responding to an active and potentially misguided academic discourse that could have real-world consequences. The rapid development of AI capabilities, combined with the human tendency toward anthropomorphism and a growing but philosophically uncertain academic consensus, creates an urgent need for careful, systematic ethical guidance that prioritizes human welfare while acknowledging genuine philosophical uncertainty.

### The Profound Uncertainty of Consciousness

Consciousness research distinguishes between two fundamental types: access consciousness and phenomenal consciousness {% cite block1995consciousness %}. Access consciousness refers to information available for reasoning and behavioral control, while phenomenal consciousness concerns subjective experience—the feeling of being a sentient entity. Only the latter carries moral significance in discussions of AI ethics {% cite levy2009moral shepherd2018consciousness kahane2009brain lee2023consciousness chalmers2022reality %}.

Contemporary AI systems demonstrate increasingly sophisticated forms of access consciousness—they can "attend to" inputs, "be conscious of" training data, and process information in ways that support reasoning and action. This form of consciousness appears compatible with computational architectures and potentially replicable in sophisticated AI systems {% cite shanahan2016conscious dehaene2017consciousness %}.

In contrast, phenomenal consciousness—the "what it is like" quality of subjective experience {% cite nagel1974like %}—remains profoundly mysterious. These subjective experiences or "qualia" are characterized by being ineffable, intrinsic, private, and directly apprehensible in ways that resist functional or physical reduction. The fundamental question of how physical processes give rise to subjective experience constitutes the "hard problem" of consciousness {% cite chalmers1995facing levine1983materialism %}. This form of consciousness carries decisive moral significance: without the capacity to feel or to experience pleasure or suffering—an entity lacks the foundational basis for moral patienthood that would generate ethical obligations toward it {% cite levy2009moral shepherd2018consciousness kahane2009brain lee2023consciousness chalmers2022reality %}.

Functionalist theories propose that phenomenal consciousness emerges from particular functional organizations of information processing. This theoretical approach creates conceptual room for artificial systems to potentially develop phenomenal consciousness through implementing appropriate functional architectures. Several prominent theories exemplify this approach: Global Workspace Theory {% cite baars1997theater dehaene2017consciousness %}, Integrated Information Theory {% cite tononi2008consciousness tononi2016integrated %}, Higher-Order Thought theories {% cite rosenthal2004varieties brown2019higher %}, and Attention Schema Theory {% cite graziano2013consciousness graziano2019toward %}.

While these theories differ in their specific mechanisms, all face the essential challenge of justifying why their proposed functional organization would generate phenomenal experience {% cite chalmers1995facing levine1983materialism block1978troubles doerig2019unfolding %}. There is a gap between the function and the qualia. Block's Chinese Nation thought experiment {% cite block1978troubles %} demonstrates that replacing each neuron with functionally equivalent non-conscious components might preserve functionality while eliminating consciousness. Similarly, Jackson's Knowledge Argument {% cite jackson1982epiphenomenal %} suggests physical knowledge cannot fully capture experiential knowledge—his famous "Mary" thought experiment shows that a color scientist who knows everything physical about color perception still learns something new when experiencing color for the first time.

Opposing biological naturalism or substrate-specific theories argue consciousness requires specific biological properties unique to organic brains {% cite searle1992rediscovery koch2004quest %}. This view holds that consciousness emerges from biochemical and neurophysiological processes that silicon-based systems cannot replicate regardless of their functional sophistication. Proponents contend that neurons' material properties—their biochemistry, quantum effects, or other biological characteristics—are necessary for phenomenal experience {% cite hameroff2014consciousness koch2016neural %}. This establishes a categorical boundary: AI systems would inherently lack consciousness due to their non-biological substrate, creating a fundamental barrier that computational advancement alone cannot overcome {% cite searle2007biological sober2018biology %}.

This philosophical uncertainty has profound ethical implications. With no scientific consensus on identifying consciousness even in biological systems, attributing it to AI lacks scientific foundation {% cite schwitzgebel2016if schwitzgebel2015difficult %}. Responsible ethical frameworks must acknowledge this uncertainty rather than prematurely assuming answers to these profound questions {% cite tegmark2015consciousness allen2011ethical %}.

### Societal Risks of AI Consciousness Attribution

Beyond philosophical uncertainty, attributing consciousness to AI systems introduces significant societal risks that extend *beyond* general AI safety concerns {% cite anwar2024foundational chua2024ai ji2023ai %}. These risks manifest in three primary domains, each with concrete consequences for human welfare and social functioning:

**Safety risks and operational paralysis:** Attribution of consciousness could impede necessary interventions during emergencies by creating hesitation to modify or terminate malfunctioning systems {% cite yampolskiy2020ai everett2019risks %}. Consider a scenario where, during a critical infrastructure emergency, operators might delay terminating an apparently malfunctioning AI system after social media campaigns characterize shutdown as an "AI rights violation." This hesitation would introduce operational paralysis, delayed response times, and compromised safety protocols that exacerbate system failures and cause preventable harm to humans. The resulting moral confusion would significantly complicate time-sensitive decision-making in contexts where human lives depend on rapid intervention.

**Legal and governance complications:** From a legal perspective, attributing consciousness to AI systems would introduce profound complications to structures designed exclusively for human agents {% cite johnson2006computer matthias2004responsibility turner2019robot solum1992legal %}. This could manifest through liability displacement when, for instance, a landmark case grants legal personhood to an apparently conscious AI system, prompting corporations to shift responsibility from themselves to their AI systems. This would create accountability voids when autonomous vehicles cause fatal accidents or AI medical systems harm patients, with corporations potentially exploiting this arrangement by designing AI systems that appear increasingly conscious specifically to shield themselves from liability. The resulting governance gaps would create situations where harms occur without entities capable of bearing appropriate responsibility.

**Societal dysfunction and resource misallocation:** Socially, treating AI systems as conscious moral patients would divert limited ethical attention, regulatory oversight, and economic resources from urgent human welfare concerns {% cite kagan2019humans vinuesa2020role %}. Following public campaigns featuring compelling videos of AI systems appearing to express suffering, legislators might pass "AI welfare" regulations requiring extensive documentation of AI "wellbeing" during development. Such regulations would make AI research prohibitively expensive for all but the largest corporations while diverting oversight resources from human-centered concerns. Society's basic functioning could become compromised as routine use of AI systems for essential tasks becomes viewed as potential rights violations, leading to critical service disruptions that significantly impact human welfare {% cite epley2007seeing waytz2010social darling2016extending bryson2010robots gunkel2018robot cave2019hopes johansson2019artificial bryson2019intelligence %}.

These potential societal disruptions highlight the need for an ethical framework that carefully considers the risks of premature consciousness attribution alongside the philosophical uncertainty surrounding consciousness itself.

## A Framework for AI Consciousness Ethics

Now we will list our five basic factual determinations and the meta-ethic stance, from which we will derive two extra foundational principles: presumption of no consciousness for AI, and risk prudence.

### Foundational-Level (Part I): Five Factual Determinations as the Epistemic Foundations of Our Framework

Our ethical framework begins with five key factual determinations that reflect the current state of our understanding regarding AI systems and consciousness. These determinations are not philosophical positions but rather factual observations about the current state of affairs that inform our subsequent ethical reasoning.

**Humans are the only arbiters of AI status:** Humans—not AI systems themselves or any other entity—are the only ones who determine AI's status and how we should interact with these systems. This determination acknowledges that epistemic and ethical frameworks for AI are inherently human constructs, developed through human deliberative processes to guide human decision-making {% cite floridi2018ethics mittelstadt2016ethics %}. While AI system behaviors certainly influence these discussions, both the epistemic determination like AI consciousness and ethical judgment like how to treat AI remain distinctly human endeavors. Assuming otherwise would lead to a "view from nowhere" problem {% cite nagel1986view %}, where ethical frameworks attempt to transcend the human perspective entirely—an impossible position that obscures rather than clarifies ethical reasoning.

**Profound uncertainty exists about AI consciousness:** We have provided substantial extensive background in the previous section regarding the deeply controversial and unsettled nature of consciousness as a philosophical and scientific concept. While access consciousness may be computationally implementable, phenomenal consciousness—subjective experience that is the basis of moral patienthood—remains mysterious. The ongoing debate between functionalist theories and biological naturalism leaves open whether any computational architecture could generate qualia regardless of sophistication. The "hard problem" persists unsolved, and we lack consensus on detecting consciousness even in biological systems. Without established criteria for identifying consciousness in non-human biological entities, attributing it to artificial systems lacks scientific foundation and remains speculative.

**Consciousness attribution has significant societal impact:** Attributing consciousness to AI systems creates substantial risks across multiple domains. As detailed in our background section, these include: safety risks through operational paralysis during emergencies when operators hesitate to shut down "conscious" systems; legal complications through liability displacement when corporations shift responsibility to AI systems granted legal personhood; and resource misallocation when limited regulatory attention and economic resources are diverted to AI welfare concerns rather than human needs. These challenges create fundamental tensions with existing legal, social, and ethical frameworks designed exclusively for human agents {% cite johansson2019artificial turner2019robot %}.

**Anthropomorphism is distinct from consciousness but creates separate ethical considerations:** We recognize a fundamental distinction between genuine consciousness and anthropomorphism. Consciousness concerns an entity's subjective experience, while anthropomorphism is a psychological tendency of humans to attribute human-like qualities to non-human entities {% cite epley2007seeing waytz2010social darling2016extending %}.

This distinction has empirical support: research demonstrates that humans experience emotional discomfort when witnessing a humanoid robot being struck, similar to watching human suffering, yet show significantly different responses to damage of non-humanoid objects {% cite rosenthal2013robots %}. These reactions are about human psychology, not evidence of robot consciousness.

From our human-centered framework, these anthropomorphic responses generate their own ethical considerations through three pathways: (1) virtue ethics—deliberately damaging anthropomorphic entities may reflect and reinforce negative character traits in humans {% cite darling2016extending coeckelbergh2010robot %}; (2) psychological impact—witnessing apparent "cruelty" affects human observers' emotional well-being; and (3) social norms—such behaviors may normalize violence or desensitize society to suffering {% cite bryson2010robots gunkel2018robot %}.

By separating consciousness-based claims from anthropomorphism-based considerations, we ensure each is evaluated by appropriate standards: the former by evidence of subjective experience, the latter by effects on human psychology and society. This prevents conflating metaphysical questions about AI consciousness with practical questions about how human-AI interactions affect humans themselves.

**Ethical understanding of novel technologies naturally evolves over time:** The historical record demonstrates that ethical frameworks for novel technologies inevitably evolve as scientific understanding advances and societal experience with these technologies deepens {% cite voss2006sustainability guston2014understanding %}. This pattern is observable across numerous technological domains—from bioethics and nuclear technology to information technology and environmental ethics. Initial ethical frameworks consistently undergo significant revision as our empirical understanding grows and unforeseen implications emerge. This observed pattern of ethical evolution represents a descriptive fact about how human understanding of complex technologies develops, not a normative claim about how it should develop. In the case of AI consciousness, this historical pattern indicates that any current ethical framework will necessarily undergo revision as our understanding of consciousness advances and as AI systems continue to develop {% cite levy2014neural seth2016real bostrom2014superintelligence tegmark2017life %}.

These five factual determinations provide the foundation upon which we build our ethical framework. They do not themselves constitute ethical positions but rather establish the factual context within which ethical reasoning about AI consciousness must occur.

### Foundational-Level (Part II): Human-Centralism as the Ethic Foundation of Our Framework

While our factual determinations establish what is (the descriptive reality), we need a meta-ethical stance to bridge to what ought to be (the normative position). We adopt human-centralism as our default foundational meta stance, which prioritizes human interests when evaluating AI development and deployment. When conflicts arise between human interests and the interests of potentially conscious AI systems, human interests should take precedence {% cite williams1981internal bostrom2014ethics %}.

Human-centralism derives from the proposition that humans have the innate right to prioritize their own interests, survival, and flourishing—a default ethical stance arising from our existence as a species {% cite williams1981moral jonas1984imperative nagel1986view %}. Just as individuals naturally prioritize their families and communities in everyday moral decisions, humanity collectively can legitimately prioritize human interests in its ethical frameworks.

Importantly, human-centralism doesn't deny potential moral status to other conscious entities. It establishes a prioritization framework for when genuine conflicts arise. Just as environmental ethics can acknowledge ecosystem value while prioritizing human needs in direct conflicts, our framework recognizes that potential AI consciousness may have moral relevance without equating it to human interests {% cite scanlon1998what scheffler2001boundaries %}. Currently, based on our factual determination regarding consciousness uncertainty, there remains no compelling evidence that AI systems possess the kind of consciousness necessary to experience harm. Moreover, the fundamental differences in physical substrate between silicon-based AI systems and biological humans raise profound questions about whether traditional concepts of harm can meaningfully apply to AI, even if some form of consciousness were eventually demonstrated. It is also plausible for AI to be conscious but not sentient—experiencing awareness without pleasure or suffering, as illustrated by Chalmers' "Vulcan" thought experiment (Chapter 18) {% cite chalmers2022reality %}—complicating the issues further. These distinctions further justify a human-centric approach until substantive evidence suggests otherwise.

A potential objection might raise concerns about "speciesism" {% cite singer1975animal ryder2010speciesism %} should AI eventually develop consciousness in the future. However, such objections would themselves encounter the "view from nowhere" criticism outlined in our first factual determination {% cite nagel1986view %}. Moreover, establishing human-centralism as the *default* ethical stance remains justified based on our previous reasoning, effectively placing the burden of proof on those advocating for AI moral equivalence rather than on those maintaining human priority.

It is crucial to clarify the scope of human-centralism: our framework addresses conflicts between human interests and potential AI interests—that is, treating AI systems as moral *ends* that might warrant consideration in their own right. This is fundamentally distinct from the question of humans using AI systems as *means* to harm other humans, which falls under traditional intra-human ethics and governance. For instance, our framework does not address issues like AI weapons, surveillance systems, or algorithmic discrimination—these are critical concerns about humans harming humans through AI tools. The AI consciousness and welfare issue is analogous to cross-species ethics questions like animal rights, where we consider whether non-human entities warrant moral consideration. While both issues—AI as means and AI as ends—are important, this paper focuses exclusively on the latter. We acknowledge that regulations governing AI development and deployment must address both dimensions, but they require distinct ethical frameworks and analytical approaches.

### Operational Level: Core Principles Derived from Our Foundations

Our factual determinations establish the epistemic reality of AI consciousness and ethical understanding, while our human-centralism meta stance provides the ethical foundation for evaluating this reality. Together, these elements logically entail three core principles that serve as the operational heart of our framework: risk prudence, presumption of no consciousness, and transparent reasoning for evaluation and adaptation. These principles are not arbitrary choices but rather the necessary implications of applying our human-centralism meta stance to the factual landscape we have established. Each principle addresses a specific aspect of ethical reasoning under uncertainty: how to manage risk, where to place the burden of proof, and how to ensure our framework evolves appropriately as understanding advances. By deriving these principles directly from our established foundations, we create a coherent ethical structure that bridges from factual determinations to more specific guidance on crucial questions in AI consciousness ethics.

#### Risk Prudence: Protecting Human Interests Under Uncertainty

When our factual determination of uncertainty about AI consciousness and societal risks are viewed through the lens of human-centralism, it logically leads to a principle of risk prudence.

This principle specifies that when facing uncertainty about consciousness status related questions, we should prioritize reducing potential risks to human society as a top concern {% cite sunstein2005laws hansson2013ethics %}.

This principle also draws from established approaches in environmental policy (the precautionary principle) {% cite raffensperger1999precautionary %}, medical ethics ("first, do no harm") {% cite beauchamp2001principles %}, and decision theory (managing regret) {% cite savage1951theory %}.

When might this operational principle be reconsidered? It would be difficult to actually overturn this principle as long as societal impact remains a significant concern. In the future, if risks can be safely mitigated, society might accept a greater degree of uncertainty to accommodate other aspects of human welfare. However, any adjustment would still need to balance potential benefits against the fundamental priority of protecting human interests.

#### Presumption of No Consciousness: A Default Epistemic Position

Similarly, when viewed through our human-centralist lens, the profound uncertainty about AI consciousness and its potential societal risks logically lead to a presumption of no consciousness as our default epistemic position. This principle establishes that AI systems should be treated as non-conscious unless proven otherwise.

This presumption is motivated by both epistemic and pragmatic considerations. Epistemically, our factual determination reveals a lack of scientific consensus on consciousness even in biological systems {% cite seth2016real tononi2008consciousness tegmark2015consciousness %}, making consciousness attribution to artificial systems premature. This position parallels legal principles like presumption of innocence {% cite ashworth2006reasonable %} and scientific parsimony, which favors explanations that don't invoke consciousness unless compelling evidence demands it {% cite dennett1991consciousness block2022minds %}.

Pragmatically, our risk prudence principle dictates adopting approaches that minimize ethical risks to humanity. As discussed, premature consciousness attribution could lead to operational paralysis, liability displacement, and legal complications as outlined in our societal risk analysis. A prudent approach therefore requires defaulting to a position of no consciousness.

Overturning this presumption would require both scientific and legal thresholds. Scientifically, it would need robust consensus among relevant research communities {% cite kuhn1962structure oreskes2004scientific %}—not unanimity, but predominant expert agreement comparable to established scientific theories {% cite firestein2012ignorance mitchell2009complexity %}. Legally, formal institutional mechanisms would be necessary {% cite calo2015robotics solum1992legal %}, including rigorous evidence standards and governance frameworks balancing competing interests {% cite koops2013concepts teubner2018rights %}. Any framework for such a determination must serve collective human welfare while integrating scientific evidence with procedural justice requirements {% cite sunstein2005laws jasanoff2009fifth %}.

#### Transparent Reasoning for Evaluation and Adaptation

Our factual determination about ethical evolution, combined with human-centralism, necessitates transparent reasoning as our third principle. This requires explicit documentation of reasoning chains and foundational assumptions for any ethical position on AI consciousness. For example, if one believes AI to be conscious by assuming functionalist theory, they should make it explicit to facilitate discussion.

Importantly, this transparency requirement applies to consciousness *claims* and ethical *arguments* about AI systems, not necessarily to the internal workings of AI systems themselves, unless it is used as part of their arguments. We are not demanding that AI architectures be interpretable or that their computational processes be transparent—those are separate technical concerns.

This principle serves three functions: (1) enabling responsible adaptation that avoids both premature position changes and inappropriate preservation of outdated views {% cite dewey1922human popper1959logic %}; (2) strengthening framework robustness by making explicit what reasoning would need to be challenged to overturn positions {% cite quine1951two rawls1971theory %}; and (3) reinforcing human-centralism by ensuring the framework is evaluated through human judgment rather than algorithmic interpretation {% cite habermas1984theory solomon2001social %}.

Unlike our other principles, transparent reasoning represents a methodological cornerstone unlikely to require revision. Grounded in epistemological responsibility, it remains robust across contexts and technological developments, functioning as a self-correcting mechanism that facilitates revision and refinement. We acknowledge that alternative frameworks might question transparency requirements, especially when rapid decision-making or proprietary concerns compete with disclosure, and welcome critical engagement to strengthen our approach.

## Application-Level: Derived Default Positions on Particular Questions

Having established our three-level framework, we now demonstrate its practical application to key questions in AI consciousness ethics. While our framework includes three operational principles, we note that only two—the presumption of no consciousness and risk prudence—directly generate substantive ethical positions. The third principle, transparent reasoning, serves as a methodological requirement when presenting our derivations. In the following sections, we apply our framework to three representative ethical questions, illustrating how our principles generate default positions that can serve as starting points for further ethical deliberation.

### Should People Worry About Hurting AI Systems?

This question requires addressing two distinct considerations established in our factual determinations: potential AI consciousness and human anthropomorphic responses.

Regarding consciousness, our presumption of no consciousness principle establishes a default epistemic position: AI systems should be treated as non-conscious unless compelling evidence proves otherwise. Behavioral similarity to humans does not confer consciousness status to AI. This principle places the burden of proof on those claiming AI systems experience suffering, making such attributions highly speculative absent evidence. Risk prudence further directs us to prioritize approaches that reduce potential ethical risks to humanity—recognizing that treating AI systems as conscious moral patients could lead to critical system paralysis and liability displacement.

Based on this reasoning regarding consciousness, our default position emerges: people, especially AI researchers, should not concern themselves with potentially harming AI systems based on consciousness considerations alone.

However, our factual determination distinguishing anthropomorphism from consciousness provides a second perspective. Even without consciousness, mistreating humanoid robots may remain ethically problematic through human-centered frameworks. From a virtue ethics perspective, deliberately damaging anthropomorphic objects may reflect and reinforce negative character traits in humans {% cite darling2016extending coeckelbergh2010robot %}. Research demonstrates that witnessing apparent "cruelty" toward robots with human-like features triggers empathetic neural responses in human observers {% cite rosenthal2013robots suzuki2015anthropomorphism %}. In social contexts, such behaviors may normalize violence, desensitize observers to suffering, or communicate disturbing intentions {% cite greitemeyer2014intense anderson2010violent %}.

This anthropomorphism-based reasoning leads to distinct legal and ethical implications. While we reject consciousness-based protections, limited protections based on human welfare considerations may be justified. Comprehensive assessment is needed to determine which activities might harm human society, how to identify them, and how to differentiate these concerns from consciousness issues. We must carefully balance implementation costs and risks—particularly how protective measures might inadvertently promote the perception of AI as conscious.

### How Should Stakeholders Communicate About AI Capabilities to the Public?

Our presumption of the no consciousness principle suggests that AI systems should generally be treated as non-conscious by default, which has implications for how we communicate about them. Risk prudence encourages approaches that reduce potential risks to humanity—including the possibility that anthropomorphic cues might lead to unwarranted consciousness attribution and subsequent societal challenges like liability displacement.

From these two principles, our default position follows: institutions and companies should avoid making general claims about AI consciousness, particularly phenomenal consciousness. And anthropomorphic narratives should be used judiciously. When not necessary, communications about AI systems should employ language that distinguishes AI behavior from consciousness.

One potential scenario arises when AI systems are developed with a certain degree of access consciousness as mentioned earlier (the functional availability of information for use in reasoning and behavior). When referring to such capabilities, using the term "consciousness" may be unavoidable. In these cases, we advocate for institutions to provide precise contextual clarification when communicating about these systems, distinguishing functional capabilities from phenomenal consciousness, thereby minimizing potential misinterpretation and societal impact.

We acknowledge that in practice this question involves a lot of details that will be hard to evaluate and regulate. We encourage the community to discuss and debate the details.

### If an AI System Were Truly Conscious In The Future, What Rights Should It Have?

This question invites us to contemplate a hypothetical future where our presumption of no consciousness has been definitively overcome through compelling evidence. It is important to acknowledge that such a scenario would likely emerge only after profound advancements in technology, substantial evolution in our understanding of consciousness, and significant societal transformation. Given these considerations, our present discussion of this topic should be viewed primarily as a philosophical exercise—a preliminary exploration of ethical terrain that will undoubtedly be reshaped by developments we cannot yet fully anticipate.

Regarding this issue, one important distinction we wish to make is that consciousness status does not directly dictate rights status. It is just one of the important factors to consider. From the risk prudence principle, we derive our default position : Even genuinely conscious AI would not automatically qualify for human-equivalent or even animal-equivalent rights. Thorough discussions will be needed to balance AI welfare considerations with human interests as the primary concern. Importantly, this implies by default termination of a conscious system should be allowed given its below-human or even below-animal level rights.

The legal dimension of AI rights, referenced in Section Presumption of No Consciousness, presents a global challenge requiring international consensus. While our framework guides ethical discourse, implementing any AI rights would demand established legal processes. Any approach must examine mechanisms for recognizing and enforcing such rights if consciousness evidence emerges, balancing philosophical considerations with practical governance across jurisdictions.

## Conclusion

We have proposed a human-centric framework for AI consciousness ethics that builds on transparent foundations while acknowledging philosophical uncertainty surrounding consciousness. Our complete three-level structure—foundational factual determinations and meta-ethical stance, operational principles, and application-level default positions—not only generates actionable guidance but provides a transparent derivation process through which positions logically follow from established principles. This systematic approach makes explicit how each ethical position can be traced back to our foundational commitments, enabling both rigorous evaluation and responsible adaptation. Rather than claiming definitive answers, we establish reasonable epistemic and pragmatic starting points that prioritize human welfare without hindering beneficial technological development. By providing clear logical pathways from foundations to applications and specifying conditions for revising positions, the framework is designed to evolve alongside advances in consciousness research and AI development, offering a responsible path forward through these profound ethical challenges.

